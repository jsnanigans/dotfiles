return {
  -- {
  --     "supermaven-inc/supermaven-nvim",
  --     event = "InsertEnter",
  --     config = function()
  --         require("supermaven-nvim").setup({
  --             -- log_level = "info", -- set to "off" to disable logging completely
  --             -- disable_inline_completion = true, -- disables inline completion for use with cmp
  --             disable_keymaps = true, -- disables built in keymaps for more manual control
  --         })
  --
  --         local suggestion = require("supermaven-nvim.completion_preview")
  --         vim.keymap.set("i", "<C-l>", function()
  --             suggestion.on_accept_suggestion()
  --         end)
  --     end,
  -- },
  --
  --
  -- {
  --   "milanglacier/minuet-ai.nvim",
  --   lazy = false,
  --   config = function()
  --     require("minuet").setup({
  --       provider = "gemini",
  --       provider_options = {
  --         gemini = {
  --           model = "gemini-2.0-flash",
  --           -- model = "gemini-2.0-flash-lite",
  --           -- system = "see [Prompt] section for the default value",
  --           -- few_shots = "see [Prompt] section for the default value",
  --           -- chat_input = "See [Prompt Section for default value]",
  --           stream = true,
  --           -- api_key = "GEMINI_API_KEY",
  --           -- optional = {},
  --         },
  --       },
  --
  --       -- Your configuration options here
  --       virtualtext = {
  --         auto_trigger_ft = {
  --           "*",
  --         },
  --         keymap = {
  --           -- accept whole completion
  --           accept = "<C-l>",
  --           -- accept one line
  --           accept_line = "<C-;>",
  --           -- accept n lines (prompts for number)
  --           -- e.g. "A-z 2 CR" will accept 2 lines
  --           -- accept_n_lines = "<A-z>",
  --           -- Cycle to prev completion item, or manually invoke completion
  --           prev = "<C-,>",
  --           -- Cycle to next completion item, or manually invoke completion
  --           next = "<C-.>",
  --           dismiss = "<C-k>",
  --         },
  --       },
  --     })
  --   end,
  -- },
  -- { "nvim-lua/plenary.nvim" },
  -- optional, if you are using virtual-text frontend, nvim-cmp is not
  -- required.
  -- { "hrsh7th/nvim-cmp" },
  -- optional, if you are using virtual-text frontend, blink is not required.
  -- { "Saghen/blink.cmp" },
  --
  --
  --
  --
  --
  --
  -- {
  --   "olimorris/codecompanion.nvim",
  --   lazy = false,
  --   dependencies = {},
  --   opts = {
  --     --Refer to: https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua
  --     strategies = {
  --       --NOTE: Change the adapter as required
  --       chat = { adapter = "copilot" },
  --       inline = { adapter = "copilot" },
  --     },
  --     opts = {
  --       log_level = "DEBUG",
  --     },
  --   },
  -- },
  --
  --
  --
  --
  --
  --
  --
  --
  -- {
  --   "huggingface/llm.nvim",
  --   lazy=false,
  --   config = function ()
  --     require('llm').setup({
  --       model = "bigcode/starcoder",
  --       url = "http://localhost:8080", -- llm-ls uses "/generate"
  --       -- cf https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate
  --       request_body = {
  --         parameters = {
  --           temperature = 0.2,
  --           top_p = 0.95,
  --         }
  --       }
  --     })
  --   end
  -- },
  --   model = "codellama:7b",
  --   url = "http://localhost:11434", -- llm-ls uses "/api/generate"
  --   -- cf https://github.com/ollama/ollama/blob/main/docs/api.md#parameters
  --   request_body = {
  --     -- Modelfile options for the model you use
  --     options = {
  --       temperature = 0.2,
  --       top_p = 0.95,
  --     },
  --   },
  -- }),
}
